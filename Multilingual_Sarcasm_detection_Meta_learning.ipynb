{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "name": "Multilingual Sarcasm detection-Meta learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/musitafa0032/Multilingual-Sarcasm-detection-via-meta-learning/blob/main/Multilingual_Sarcasm_detection_Meta_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f9e6624"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset,TensorDataset,RandomSampler,DataLoader\n",
        "import numpy as np\n",
        "#import collections\n",
        "import random\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import BertTokenizer,BertForSequenceClassification\n",
        "from copy import deepcopy\n",
        "import gc\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "id": "1f9e6624",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T3NgbK1Fqxx"
      },
      "source": [
        "# New Section"
      ],
      "id": "4T3NgbK1Fqxx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af646f5e"
      },
      "source": [
        "df_ar=pd.read_csv(\"/train_data/train.Ar.csv\")\n",
        "df_ar.dropna(axis=0,how=\"any\")\n",
        "df_en=pd.read_csv(\"/train_data/train.En.csv\")\n",
        "df_en.dropna(axis=0,how=\"any\")\n",
        "#shuffle df dataset\n",
        "df_ar=df_ar.sample(frac=1,ignore_index=True)\n",
        "tweets_ar=df_ar.tweet.to_list()\n",
        "labels_ar=df_ar.sarcastic.to_list()\n",
        "df_en=df_en.sample(frac=1,ignore_index=True)\n",
        "tweets_en=df_en.tweet.to_list()\n",
        "labels_en=df_en.sarcastic.to_list()"
      ],
      "id": "af646f5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c031cbb6"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer=BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\",do_lower_case=True)\n",
        "\n",
        "data_supports=[]\n",
        "for item in [zip(tweets_ar,labels_ar),zip(tweets_en,labels_en)]:\n",
        "    data_list=[]\n",
        "    for tweet,label in item:\n",
        "        d=dict()\n",
        "        d[\"text\"]=tweet\n",
        "        d[\"label\"]=label\n",
        "        data_list.append(d)\n",
        "    data_supports.append(data_list)\n",
        "    del data_list"
      ],
      "id": "c031cbb6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWUJQertIKoF"
      },
      "source": [
        "# New Section"
      ],
      "id": "iWUJQertIKoF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a08c4c5"
      },
      "source": [
        "class MetaTask(Dataset):\n",
        "    def __init__(self,data_lists_support,data_lists_query,tokenizer):\n",
        "        self.data_lists_support=data_lists_support\n",
        "        self.data_lists_query=data_lists_query\n",
        "        self.max_seq_length=256\n",
        "        self.tokenizer=tokenizer\n",
        "    \n",
        "    def create_feature_set(self,examples):\n",
        "        all_input_ids=torch.empty(len(examples),self.max_seq_length,dtype=torch.long)\n",
        "        all_attention_mask=torch.empty(len(examples),self.max_seq_length,dtype=torch.long)\n",
        "        all_segment_ids=torch.empty(len(examples),self.max_seq_length,dtype=torch.long)\n",
        "        all_label_ids=torch.empty(len(examples),dtype=torch.long)\n",
        "        \n",
        "        for id_,example in enumerate(examples):\n",
        "            input_ids=self.tokenizer.encode(str(example[\"text\"]))\n",
        "            attention_mask = [1]*len(input_ids)\n",
        "            segment_ids=[0]*len(input_ids)\n",
        "            \n",
        "            while len(input_ids)<self.max_seq_length:\n",
        "                input_ids.append(0)\n",
        "                attention_mask.append(0)\n",
        "                segment_ids.append(0)\n",
        "            \n",
        "            label_id=example[\"label\"]\n",
        "            all_input_ids[id_]=torch.Tensor(input_ids).to(torch.long)\n",
        "            all_attention_mask[id_]=torch.Tensor(attention_mask).to(torch.long)\n",
        "            all_segment_ids[id_]=torch.Tensor(segment_ids).to(torch.long)\n",
        "            all_label_ids[id_]=torch.Tensor([label_id]).to(torch.long)\n",
        "        tensor_set=TensorDataset(all_input_ids,all_attention_mask,all_segment_ids,all_label_ids)\n",
        "        return tensor_set\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        support_set=self.create_feature_set(self.data_lists_support[index])\n",
        "        query_set=self.create_feature_set(self.data_lists_query[index])\n",
        "        return support_set,query_set\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data_lists_support)"
      ],
      "id": "2a08c4c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd2bc7ab"
      },
      "source": [
        "class Learner(nn.Module):\n",
        "    def __init__(self,num_labels,outer_batch_size,inner_batch_size,outer_update_lr,inner_update_lr,inner_update_step,inner_update_step_eval,bert_model):\n",
        "        super(Learner,self).__init__()\n",
        "        \n",
        "        self.num_labels=num_labels\n",
        "        self.outer_batch_size = outer_batch_size\n",
        "        self.inner_batch_size = inner_batch_size\n",
        "        self.outer_update_lr  = outer_update_lr\n",
        "        self.inner_update_lr  = inner_update_lr\n",
        "        self.inner_update_step = inner_update_step\n",
        "        self.inner_update_step_eval = inner_update_step_eval\n",
        "        self.bert_model = bert_model\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        self.model = BertForSequenceClassification.from_pretrained(self.bert_model, num_labels = self.num_labels)\n",
        "        self.outer_optimizer = AdamW(self.model.parameters(), lr=self.outer_update_lr)\n",
        "        self.model.train()\n",
        "    def forward(self,batch_tasks,training =True):\n",
        "        task_accs=[]\n",
        "        sum_gradients=[]\n",
        "        num_task=len(batch_tasks)\n",
        "        num_inner_update_step=self.inner_update_step if training else self.inner_update_step_eval\n",
        "        \n",
        "        for task_id,task in enumerate(batch_tasks):\n",
        "            support=task[0]\n",
        "            query=task[1]\n",
        "            \n",
        "            fast_model=deepcopy(self.model)\n",
        "            fast_model.to(self.device)\n",
        "            support_dataloader=DataLoader(support,sampler=RandomSampler(support),\n",
        "                                          batch_size=self.inner_batch_size)\n",
        "            inner_optimizer=AdamW(fast_model.parameters(),lr=self.inner_update_lr)\n",
        "            fast_model.train()\n",
        "            \n",
        "            print('----Task',task_id,'----')\n",
        "            for i in range(0,num_inner_update_step):\n",
        "                all_loss=[]\n",
        "                for inner_step,batch in enumerate(support_dataloader):\n",
        "                    \n",
        "                    batch=tuple(t.to(self.device) for t in batch)\n",
        "                    input_ids,attention_mask,segment_ids,label_id=batch\n",
        "                    outputs=fast_model(input_ids,attention_mask,segment_ids,labels=label_id)\n",
        "                    \n",
        "                    loss=outputs[0]\n",
        "                    loss.backward()\n",
        "                    inner_optimizer.step()\n",
        "                    inner_optimizer.zero_grad()\n",
        "                    \n",
        "                    all_loss.append(loss.item())\n",
        "                    \n",
        "                if i%4 == 0:\n",
        "                    print(\"Inner Loss:\",np.mean(all_loss))\n",
        "                    \n",
        "            fast_model.to(torch.device(\"cpu\"))\n",
        "            \n",
        "            if training:\n",
        "                meta_weights=list(self.model.parameters())\n",
        "                fast_weights=list(fast_model.parameters())\n",
        "                \n",
        "                gradients=[]\n",
        "                for i,(meta_params,fast_params) in enumerate(zip(meta_weights,fast_weights)):\n",
        "                    gradient=meta_params-fast_params\n",
        "                    if task_id==0:\n",
        "                        sum_gradients.append(gradient)\n",
        "                    else:\n",
        "                        sum_gradients[i]+=gradient\n",
        "            \n",
        "            fast_model.to(self.device)\n",
        "            fast_model.eval()\n",
        "            with torch.no_grad():\n",
        "                query_dataloader = DataLoader(query, sampler=None, batch_size=len(query))\n",
        "                query_batch = iter(query_dataloader).next()\n",
        "                query_batch = tuple(t.to(self.device) for t in query_batch)\n",
        "                q_input_ids, q_attention_mask, q_segment_ids, q_label_id = query_batch\n",
        "                q_outputs = fast_model(q_input_ids, q_attention_mask, q_segment_ids, labels = q_label_id)\n",
        "\n",
        "                q_logits = F.softmax(q_outputs[1],dim=1)\n",
        "                pre_label_id = torch.argmax(q_logits,dim=1)\n",
        "                pre_label_id = pre_label_id.detach().cpu().numpy().tolist()\n",
        "                q_label_id = q_label_id.detach().cpu().numpy().tolist()\n",
        "\n",
        "                acc = accuracy_score(pre_label_id,q_label_id)\n",
        "                task_accs.append(acc)\n",
        "            \n",
        "            fast_model.to(torch.device('cpu'))\n",
        "            del fast_model, inner_optimizer\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        if training:\n",
        "            # Average gradient across tasks\n",
        "            for i in range(0,len(sum_gradients)):\n",
        "                sum_gradients[i] = sum_gradients[i] / float(num_task)\n",
        "\n",
        "            #Assign gradient for original model, then using optimizer to update its weights\n",
        "            for i, params in enumerate(self.model.parameters()):\n",
        "                params.grad = sum_gradients[i]\n",
        "\n",
        "            self.outer_optimizer.step()\n",
        "            self.outer_optimizer.zero_grad()\n",
        "            \n",
        "            del sum_gradients\n",
        "            gc.collect()\n",
        "            \n",
        "        return np.mean(task_accs)\n",
        "\n",
        "    \n"
      ],
      "id": "dd2bc7ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56c598fe"
      },
      "source": [
        "from random import shuffle\n",
        "#from collections import Counter\n",
        "import torch\n",
        "#from transformers import BertModel, BertTokenizer\n",
        "import time\n",
        "import logging\n",
        "#import argparse\n",
        "import os\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.CRITICAL)\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'#locate which place to debug when using gpu\n",
        "#from reptile import Learner\n",
        "#from task import MetaTask\n",
        "import random\n",
        "#import numpy as np"
      ],
      "id": "56c598fe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbd377df"
      },
      "source": [
        "def random_seed(value):\n",
        "    torch.backends.cudnn.deterministic=True\n",
        "    torch.manual_seed(value)\n",
        "    torch.cuda.manual_seed(value)\n",
        "    np.random.seed(value)\n",
        "    random.seed(value)\n",
        "\n",
        "def create_batch_of_tasks(taskset, is_shuffle = True, batch_size = 4):\n",
        "    idxs = list(range(0,len(taskset)))\n",
        "    if is_shuffle:\n",
        "        random.shuffle(idxs)\n",
        "    for i in range(0,len(idxs), batch_size):\n",
        "        yield [taskset[idxs[i]] for i in range(i, min(i + batch_size,len(taskset)))]\n",
        "\n"
      ],
      "id": "bbd377df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47a867a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57e84fb9-b542-43b7-e048-c7f3b3dd0ca4"
      },
      "source": [
        "learner=Learner(num_labels=2,outer_batch_size = 2,inner_batch_size = 12,outer_update_lr = 5e-5,inner_update_lr = 5e-5,inner_update_step = 10,inner_update_step_eval = 40,bert_model = 'bert-base-multilingual-cased')\n",
        "test_task=MetaTask(data_supports, data_supports,tokenizer)\n",
        "global_step=0\n",
        "for epoch in range(10):\n",
        "    train_task=MetaTask(data_supports, data_supports,tokenizer)\n",
        "    db = create_batch_of_tasks(train_task, is_shuffle = True, batch_size = 2)\n",
        "    for step, task_batch in enumerate(db):\n",
        "        f = open('log.txt', 'a')\n",
        "        acc = learner(task_batch)\n",
        "        print('Step:', step, '\\ttraining Acc:', acc)\n",
        "        if global_step % 20 == 0:\n",
        "            random_seed(123)\n",
        "            print(\"\\n-----------------Testing Mode-----------------\\n\")\n",
        "            db_test = create_batch_of_tasks(test_task, is_shuffle = False, batch_size = 1)\n",
        "            acc_all_test = []\n",
        "            \n",
        "            for test_batch in db_test:\n",
        "                acc = learner(test_batch, training = False)\n",
        "                acc_all_test.append(acc)\n",
        "            \n",
        "            print('Step:', step, 'Test F1:', np.mean(acc_all_test))\n",
        "            random_seed(int(time.time() % 10))\n",
        "            \n",
        "        global_step += 1\n",
        "        f.close()\n",
        "                "
      ],
      "id": "47a867a4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----Task 0 ----\n",
            "Inner Loss: 0.3809157653801864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2981a36f"
      },
      "source": [
        ""
      ],
      "id": "2981a36f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddc51f79",
        "outputId": "103b6611-fbd9-4308-b09a-86278350e351"
      },
      "source": [
        "pip install transformers"
      ],
      "id": "ddc51f79",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 18.5 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 38.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 68.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 51.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "bkqEYX0yHblq",
        "outputId": "1eba7df9-529b-4381-8b4e-8acb15407b40"
      },
      "source": [
        "pip install pandas --upgrade"
      ],
      "id": "bkqEYX0yHblq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 15.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.4 which is incompatible.\u001b[0m\n",
            "Successfully installed pandas-1.3.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swHEDdk0HyTI"
      },
      "source": [
        ""
      ],
      "id": "swHEDdk0HyTI",
      "execution_count": null,
      "outputs": []
    }
  ]
}